{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from lib import model_device_io as io\n",
    "from lib import data_loader as dl\n",
    "from lib import models as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda:1\n",
      "model loaded from models/rev_best.pth\n"
     ]
    }
   ],
   "source": [
    "trainset = dl.HotelReservationData(root='data/train')\n",
    "testset = dl.HotelReservationData(root='data/test')\n",
    "train_loader = DataLoader(trainset, batch_size=4096, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(testset, batch_size=4096, shuffle=False, num_workers=0)\n",
    "device = io.getCudaDevice(cudaNum = 1, torchSeed = 123)\n",
    "model = md.ClassifierLinear(input_size = 258).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "io.loadModel('models/rev_best.pth', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_epoch = 100, model_fn = \"rev_best.pth\", log_per_epoch = 1):\n",
    "    best_acc = test()\n",
    "    for epoch in range(train_epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, _, rev) in enumerate(train_loader):\n",
    "            data, rev = data.to(device), rev.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, rev)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (batch_idx+1) % max(1, int(len(train_loader)/log_per_epoch)) == 0:\n",
    "                print('Training Epoch: {:.2f}% ({}/{})\\tBatch: {:.2f}% ({}/{})\\tLoss: {:.6f}'.format(\n",
    "                      (epoch+1)*100./train_epoch, epoch+1, train_epoch, (batch_idx+1)*100./len(train_loader),\n",
    "                      batch_idx+1, len(train_loader), loss.item()))\n",
    "        acc = test()\n",
    "        if(best_acc < acc):\n",
    "            best_acc = acc\n",
    "            fn = os.path.join('models', model_fn)\n",
    "            io.saveModel(fn, model, optimizer)\n",
    "            \n",
    "            \n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad(): # This will free the GPU memory used for back-prop\n",
    "        for data, _, rev in test_loader:\n",
    "            data, rev = data.to(device), rev.to(device)\n",
    "            output = model(data)\n",
    "            loss += F.cross_entropy(output, rev).item()*len(data) # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(rev.view_as(pred)).sum().item()\n",
    "            \n",
    "    loss /= len(test_loader.dataset)\n",
    "    acc = 1.*correct/len(test_loader.dataset)\n",
    "    print('***Validation Results***\\tLoss: {:.4f},\\tAccuracy: {:.2f}% ({}/{})\\n'.format(\n",
    "          loss, acc*100., correct, len(test_loader.dataset)))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Validation Results***\tLoss: 1518.3077,\tAccuracy: 69.32% (6932/10000)\n",
      "\n",
      "Training Epoch: 0.02% (1/5000)\tBatch: 100.00% (20/20)\tLoss: 1425.119873\n",
      "***Validation Results***\tLoss: 1801.9521,\tAccuracy: 66.96% (6696/10000)\n",
      "\n",
      "Training Epoch: 0.04% (2/5000)\tBatch: 100.00% (20/20)\tLoss: 1712.147827\n",
      "***Validation Results***\tLoss: 1690.7118,\tAccuracy: 68.89% (6889/10000)\n",
      "\n",
      "Training Epoch: 0.06% (3/5000)\tBatch: 100.00% (20/20)\tLoss: 1442.703857\n",
      "***Validation Results***\tLoss: 1463.3652,\tAccuracy: 68.62% (6862/10000)\n",
      "\n",
      "Training Epoch: 0.08% (4/5000)\tBatch: 100.00% (20/20)\tLoss: 1713.805176\n",
      "***Validation Results***\tLoss: 1779.1821,\tAccuracy: 44.00% (4400/10000)\n",
      "\n",
      "Training Epoch: 0.10% (5/5000)\tBatch: 100.00% (20/20)\tLoss: 1655.023682\n",
      "***Validation Results***\tLoss: 2261.0960,\tAccuracy: 38.75% (3875/10000)\n",
      "\n",
      "Training Epoch: 0.12% (6/5000)\tBatch: 100.00% (20/20)\tLoss: 1963.965576\n",
      "***Validation Results***\tLoss: 2230.6133,\tAccuracy: 38.96% (3896/10000)\n",
      "\n",
      "Training Epoch: 0.14% (7/5000)\tBatch: 100.00% (20/20)\tLoss: 1450.368042\n",
      "***Validation Results***\tLoss: 1646.1961,\tAccuracy: 47.55% (4755/10000)\n",
      "\n",
      "Training Epoch: 0.16% (8/5000)\tBatch: 100.00% (20/20)\tLoss: 1243.018066\n",
      "***Validation Results***\tLoss: 1640.9424,\tAccuracy: 68.96% (6896/10000)\n",
      "\n",
      "Training Epoch: 0.18% (9/5000)\tBatch: 100.00% (20/20)\tLoss: 1984.566650\n",
      "***Validation Results***\tLoss: 2223.5580,\tAccuracy: 66.58% (6658/10000)\n",
      "\n",
      "Training Epoch: 0.20% (10/5000)\tBatch: 100.00% (20/20)\tLoss: 2587.295654\n",
      "***Validation Results***\tLoss: 2684.1350,\tAccuracy: 66.46% (6646/10000)\n",
      "\n",
      "Training Epoch: 0.22% (11/5000)\tBatch: 100.00% (20/20)\tLoss: 2725.375977\n",
      "***Validation Results***\tLoss: 2973.9256,\tAccuracy: 65.95% (6595/10000)\n",
      "\n",
      "Training Epoch: 0.24% (12/5000)\tBatch: 100.00% (20/20)\tLoss: 2959.032715\n",
      "***Validation Results***\tLoss: 3084.1612,\tAccuracy: 65.86% (6586/10000)\n",
      "\n",
      "Training Epoch: 0.26% (13/5000)\tBatch: 100.00% (20/20)\tLoss: 2768.332031\n",
      "***Validation Results***\tLoss: 3008.9403,\tAccuracy: 65.95% (6595/10000)\n",
      "\n",
      "Training Epoch: 0.28% (14/5000)\tBatch: 100.00% (20/20)\tLoss: 2455.596924\n",
      "***Validation Results***\tLoss: 2745.2518,\tAccuracy: 66.33% (6633/10000)\n",
      "\n",
      "Training Epoch: 0.30% (15/5000)\tBatch: 100.00% (20/20)\tLoss: 2123.014404\n",
      "***Validation Results***\tLoss: 2293.9087,\tAccuracy: 66.59% (6659/10000)\n",
      "\n",
      "Training Epoch: 0.32% (16/5000)\tBatch: 100.00% (20/20)\tLoss: 1311.395020\n",
      "***Validation Results***\tLoss: 1670.0478,\tAccuracy: 68.97% (6897/10000)\n",
      "\n",
      "Training Epoch: 0.34% (17/5000)\tBatch: 100.00% (20/20)\tLoss: 1156.755249\n",
      "***Validation Results***\tLoss: 1786.0386,\tAccuracy: 43.18% (4318/10000)\n",
      "\n",
      "Training Epoch: 0.36% (18/5000)\tBatch: 100.00% (20/20)\tLoss: 2605.987061\n",
      "***Validation Results***\tLoss: 3084.0267,\tAccuracy: 35.90% (3590/10000)\n",
      "\n",
      "Training Epoch: 0.38% (19/5000)\tBatch: 100.00% (20/20)\tLoss: 3365.750000\n",
      "***Validation Results***\tLoss: 3849.4260,\tAccuracy: 35.32% (3532/10000)\n",
      "\n",
      "Training Epoch: 0.40% (20/5000)\tBatch: 100.00% (20/20)\tLoss: 3785.673340\n",
      "***Validation Results***\tLoss: 3973.8493,\tAccuracy: 35.26% (3526/10000)\n",
      "\n",
      "Training Epoch: 0.42% (21/5000)\tBatch: 100.00% (20/20)\tLoss: 3401.272461\n",
      "***Validation Results***\tLoss: 3433.7792,\tAccuracy: 35.67% (3567/10000)\n",
      "\n",
      "Training Epoch: 0.44% (22/5000)\tBatch: 100.00% (20/20)\tLoss: 2336.127686\n",
      "***Validation Results***\tLoss: 2234.3107,\tAccuracy: 38.30% (3830/10000)\n",
      "\n",
      "Training Epoch: 0.46% (23/5000)\tBatch: 100.00% (20/20)\tLoss: 1322.750977\n",
      "***Validation Results***\tLoss: 1530.2004,\tAccuracy: 68.85% (6885/10000)\n",
      "\n",
      "Training Epoch: 0.48% (24/5000)\tBatch: 100.00% (20/20)\tLoss: 2200.805176\n",
      "***Validation Results***\tLoss: 2534.1373,\tAccuracy: 66.55% (6655/10000)\n",
      "\n",
      "Training Epoch: 0.50% (25/5000)\tBatch: 100.00% (20/20)\tLoss: 3135.356689\n",
      "***Validation Results***\tLoss: 3424.5858,\tAccuracy: 65.46% (6546/10000)\n",
      "\n",
      "Training Epoch: 0.52% (26/5000)\tBatch: 100.00% (20/20)\tLoss: 3857.604492\n",
      "***Validation Results***\tLoss: 4136.9641,\tAccuracy: 64.94% (6494/10000)\n",
      "\n",
      "Training Epoch: 0.54% (27/5000)\tBatch: 100.00% (20/20)\tLoss: 4640.774902\n",
      "***Validation Results***\tLoss: 4665.5344,\tAccuracy: 64.75% (6475/10000)\n",
      "\n",
      "Training Epoch: 0.56% (28/5000)\tBatch: 100.00% (20/20)\tLoss: 4758.499512\n",
      "***Validation Results***\tLoss: 5007.9967,\tAccuracy: 64.58% (6458/10000)\n",
      "\n",
      "Training Epoch: 0.58% (29/5000)\tBatch: 100.00% (20/20)\tLoss: 5220.194824\n",
      "***Validation Results***\tLoss: 5162.2036,\tAccuracy: 64.50% (6450/10000)\n",
      "\n",
      "Training Epoch: 0.60% (30/5000)\tBatch: 100.00% (20/20)\tLoss: 5221.229980\n",
      "***Validation Results***\tLoss: 5124.1833,\tAccuracy: 64.49% (6449/10000)\n",
      "\n",
      "Training Epoch: 0.62% (31/5000)\tBatch: 100.00% (20/20)\tLoss: 4545.663574\n",
      "***Validation Results***\tLoss: 4890.2952,\tAccuracy: 64.63% (6463/10000)\n",
      "\n",
      "Training Epoch: 0.64% (32/5000)\tBatch: 100.00% (20/20)\tLoss: 4461.212891\n",
      "***Validation Results***\tLoss: 4458.7668,\tAccuracy: 64.81% (6481/10000)\n",
      "\n",
      "Training Epoch: 0.66% (33/5000)\tBatch: 100.00% (20/20)\tLoss: 3640.686768\n",
      "***Validation Results***\tLoss: 3830.5396,\tAccuracy: 65.09% (6509/10000)\n",
      "\n",
      "Training Epoch: 0.68% (34/5000)\tBatch: 100.00% (20/20)\tLoss: 2984.781738\n",
      "***Validation Results***\tLoss: 3011.6862,\tAccuracy: 65.65% (6565/10000)\n",
      "\n",
      "Training Epoch: 0.70% (35/5000)\tBatch: 100.00% (20/20)\tLoss: 1618.787842\n",
      "***Validation Results***\tLoss: 2018.9127,\tAccuracy: 66.52% (6652/10000)\n",
      "\n",
      "Training Epoch: 0.72% (36/5000)\tBatch: 100.00% (20/20)\tLoss: 814.280884\n",
      "***Validation Results***\tLoss: 1236.3850,\tAccuracy: 56.25% (5625/10000)\n",
      "\n",
      "Training Epoch: 0.74% (37/5000)\tBatch: 100.00% (20/20)\tLoss: 2711.275879\n",
      "***Validation Results***\tLoss: 3235.0461,\tAccuracy: 35.74% (3574/10000)\n",
      "\n",
      "Training Epoch: 0.76% (38/5000)\tBatch: 100.00% (20/20)\tLoss: 4670.996582\n",
      "***Validation Results***\tLoss: 4849.9343,\tAccuracy: 34.82% (3482/10000)\n",
      "\n",
      "Training Epoch: 0.78% (39/5000)\tBatch: 100.00% (20/20)\tLoss: 5817.998535\n",
      "***Validation Results***\tLoss: 5856.7350,\tAccuracy: 34.68% (3468/10000)\n",
      "\n",
      "Training Epoch: 0.80% (40/5000)\tBatch: 100.00% (20/20)\tLoss: 6052.212891\n",
      "***Validation Results***\tLoss: 6235.0592,\tAccuracy: 34.65% (3465/10000)\n",
      "\n",
      "Training Epoch: 0.82% (41/5000)\tBatch: 100.00% (20/20)\tLoss: 5835.499512\n",
      "***Validation Results***\tLoss: 5972.9919,\tAccuracy: 34.66% (3466/10000)\n",
      "\n",
      "Training Epoch: 0.84% (42/5000)\tBatch: 100.00% (20/20)\tLoss: 5226.315918\n",
      "***Validation Results***\tLoss: 5062.4538,\tAccuracy: 34.82% (3482/10000)\n",
      "\n",
      "Training Epoch: 0.86% (43/5000)\tBatch: 100.00% (20/20)\tLoss: 3261.206543\n",
      "***Validation Results***\tLoss: 3517.1515,\tAccuracy: 35.53% (3553/10000)\n",
      "\n",
      "Training Epoch: 0.88% (44/5000)\tBatch: 100.00% (20/20)\tLoss: 1371.637329\n",
      "***Validation Results***\tLoss: 1419.0451,\tAccuracy: 44.47% (4447/10000)\n",
      "\n",
      "Training Epoch: 0.90% (45/5000)\tBatch: 100.00% (20/20)\tLoss: 1669.250366\n",
      "***Validation Results***\tLoss: 1850.7975,\tAccuracy: 66.58% (6658/10000)\n",
      "\n",
      "Training Epoch: 0.92% (46/5000)\tBatch: 100.00% (20/20)\tLoss: 2792.795166\n",
      "***Validation Results***\tLoss: 3111.0482,\tAccuracy: 65.54% (6554/10000)\n",
      "\n",
      "Training Epoch: 0.94% (47/5000)\tBatch: 100.00% (20/20)\tLoss: 3967.463623\n",
      "***Validation Results***\tLoss: 4207.2433,\tAccuracy: 64.94% (6494/10000)\n",
      "\n",
      "Training Epoch: 0.96% (48/5000)\tBatch: 100.00% (20/20)\tLoss: 4946.914551\n",
      "***Validation Results***\tLoss: 5128.8659,\tAccuracy: 64.54% (6454/10000)\n",
      "\n",
      "Training Epoch: 0.98% (49/5000)\tBatch: 100.00% (20/20)\tLoss: 5457.306152\n",
      "***Validation Results***\tLoss: 5880.9476,\tAccuracy: 64.39% (6439/10000)\n",
      "\n",
      "Training Epoch: 1.00% (50/5000)\tBatch: 100.00% (20/20)\tLoss: 6472.277832\n",
      "***Validation Results***\tLoss: 6466.9232,\tAccuracy: 64.33% (6433/10000)\n",
      "\n",
      "Training Epoch: 1.02% (51/5000)\tBatch: 100.00% (20/20)\tLoss: 6471.378906\n",
      "***Validation Results***\tLoss: 6888.0087,\tAccuracy: 64.35% (6435/10000)\n",
      "\n",
      "Training Epoch: 1.04% (52/5000)\tBatch: 100.00% (20/20)\tLoss: 6756.762207\n",
      "***Validation Results***\tLoss: 7142.4315,\tAccuracy: 64.36% (6436/10000)\n",
      "\n",
      "Training Epoch: 1.06% (53/5000)\tBatch: 100.00% (20/20)\tLoss: 7138.877441\n",
      "***Validation Results***\tLoss: 7228.2607,\tAccuracy: 64.36% (6436/10000)\n",
      "\n",
      "Training Epoch: 1.08% (54/5000)\tBatch: 100.00% (20/20)\tLoss: 7230.755371\n",
      "***Validation Results***\tLoss: 7141.9556,\tAccuracy: 64.36% (6436/10000)\n",
      "\n",
      "Training Epoch: 1.10% (55/5000)\tBatch: 100.00% (20/20)\tLoss: 6783.808105\n",
      "***Validation Results***\tLoss: 6880.9833,\tAccuracy: 64.33% (6433/10000)\n",
      "\n",
      "Training Epoch: 1.12% (56/5000)\tBatch: 100.00% (20/20)\tLoss: 6580.725098\n",
      "***Validation Results***\tLoss: 6444.1162,\tAccuracy: 64.32% (6432/10000)\n",
      "\n",
      "Training Epoch: 1.14% (57/5000)\tBatch: 100.00% (20/20)\tLoss: 5921.720703\n",
      "***Validation Results***\tLoss: 5831.6995,\tAccuracy: 64.43% (6443/10000)\n",
      "\n",
      "Training Epoch: 1.16% (58/5000)\tBatch: 100.00% (20/20)\tLoss: 5138.104004\n",
      "***Validation Results***\tLoss: 5047.3324,\tAccuracy: 64.64% (6464/10000)\n",
      "\n",
      "Training Epoch: 1.18% (59/5000)\tBatch: 100.00% (20/20)\tLoss: 4082.258057\n",
      "***Validation Results***\tLoss: 4099.1528,\tAccuracy: 65.02% (6502/10000)\n",
      "\n",
      "Training Epoch: 1.20% (60/5000)\tBatch: 100.00% (20/20)\tLoss: 2775.512695\n",
      "***Validation Results***\tLoss: 3000.4320,\tAccuracy: 65.63% (6563/10000)\n",
      "\n",
      "Training Epoch: 1.22% (61/5000)\tBatch: 100.00% (20/20)\tLoss: 1630.729858\n",
      "***Validation Results***\tLoss: 1774.5676,\tAccuracy: 66.57% (6657/10000)\n",
      "\n",
      "Training Epoch: 1.24% (62/5000)\tBatch: 100.00% (20/20)\tLoss: 609.394409\n",
      "***Validation Results***\tLoss: 724.1799,\tAccuracy: 64.51% (6451/10000)\n",
      "\n",
      "Training Epoch: 1.26% (63/5000)\tBatch: 100.00% (20/20)\tLoss: 2711.429443\n",
      "***Validation Results***\tLoss: 2923.1582,\tAccuracy: 35.76% (3576/10000)\n",
      "\n",
      "Training Epoch: 1.28% (64/5000)\tBatch: 100.00% (20/20)\tLoss: 4790.158691\n",
      "***Validation Results***\tLoss: 4929.3971,\tAccuracy: 34.84% (3484/10000)\n",
      "\n",
      "Training Epoch: 1.30% (65/5000)\tBatch: 100.00% (20/20)\tLoss: 6174.500000\n",
      "***Validation Results***\tLoss: 6422.5061,\tAccuracy: 34.62% (3462/10000)\n",
      "\n",
      "Training Epoch: 1.32% (66/5000)\tBatch: 100.00% (20/20)\tLoss: 7137.942383\n",
      "***Validation Results***\tLoss: 7397.4636,\tAccuracy: 34.59% (3459/10000)\n",
      "\n",
      "Training Epoch: 1.34% (67/5000)\tBatch: 100.00% (20/20)\tLoss: 7944.387695\n",
      "***Validation Results***\tLoss: 7850.7347,\tAccuracy: 34.59% (3459/10000)\n",
      "\n",
      "Training Epoch: 1.36% (68/5000)\tBatch: 100.00% (20/20)\tLoss: 7862.325684\n",
      "***Validation Results***\tLoss: 7774.4856,\tAccuracy: 34.59% (3459/10000)\n",
      "\n",
      "Training Epoch: 1.38% (69/5000)\tBatch: 100.00% (20/20)\tLoss: 7127.578125\n",
      "***Validation Results***\tLoss: 7160.2831,\tAccuracy: 34.59% (3459/10000)\n",
      "\n",
      "Training Epoch: 1.40% (70/5000)\tBatch: 100.00% (20/20)\tLoss: 5909.983887\n",
      "***Validation Results***\tLoss: 6009.3165,\tAccuracy: 34.68% (3468/10000)\n",
      "\n",
      "Training Epoch: 1.42% (71/5000)\tBatch: 100.00% (20/20)\tLoss: 4334.480469\n",
      "***Validation Results***\tLoss: 4338.7250,\tAccuracy: 34.99% (3499/10000)\n",
      "\n",
      "Training Epoch: 1.44% (72/5000)\tBatch: 100.00% (20/20)\tLoss: 2208.009521\n",
      "***Validation Results***\tLoss: 2191.8057,\tAccuracy: 36.51% (3651/10000)\n",
      "\n",
      "Training Epoch: 1.46% (73/5000)\tBatch: 100.00% (20/20)\tLoss: 565.622620\n",
      "***Validation Results***\tLoss: 612.6566,\tAccuracy: 69.01% (6901/10000)\n",
      "\n",
      "Training Epoch: 1.48% (74/5000)\tBatch: 100.00% (20/20)\tLoss: 1684.546143\n",
      "***Validation Results***\tLoss: 1916.0381,\tAccuracy: 66.47% (6647/10000)\n",
      "\n",
      "Training Epoch: 1.50% (75/5000)\tBatch: 100.00% (20/20)\tLoss: 3095.649658\n",
      "***Validation Results***\tLoss: 3180.4001,\tAccuracy: 65.53% (6553/10000)\n",
      "\n",
      "Training Epoch: 1.52% (76/5000)\tBatch: 100.00% (20/20)\tLoss: 4139.487305\n",
      "***Validation Results***\tLoss: 4299.3898,\tAccuracy: 65.03% (6503/10000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(train_epoch = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
